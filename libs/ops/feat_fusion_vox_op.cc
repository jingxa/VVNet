/* Microsoft Research Asia, Internal Graphics * * Important: * 1. ONLY NHWC and HDHWC are supported now.*/#include "tensorflow/core/framework/op.h"#include "tensorflow/core/framework/op_kernel.h"#include "tensorflow/core/framework/shape_inference.h"#include "tensorflow/core/framework/common_shape_fns.h"#include "tensorflow/core/util/tensor_format.h"#include "feat_fusion.h"typedef float T;namespace tensorflow{REGISTER_OP("FeatFusionVox")    .Input("img_feat: float")    .Input("img_map: int32")    .Input("img_weights: float")    .Input("vox_weights: float")    .Output("vox_feat: float")    .Attr("fusion_type: int")    .SetShapeFn([](::tensorflow::shape_inference::InferenceContext* c) {        shape_inference::ShapeHandle img_feat_shape;        TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 4, &img_feat_shape));        auto batch_size_dim = c->Dim(img_feat_shape, 0);        auto in_channel_dim = c->Dim(img_feat_shape, -1);        shape_inference::ShapeHandle vox_map_shape;        TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 5, &vox_map_shape));        auto out_depth_dim = c->Dim(vox_map_shape, 1);        auto out_height_dim = c->Dim(vox_map_shape, 2);        auto out_width_dim = c->Dim(vox_map_shape, 3);        shape_inference::ShapeHandle feat_vox_shape = c->MakeShape(std::vector<shape_inference::DimensionHandle>{                batch_size_dim, out_depth_dim, out_height_dim, out_width_dim, in_channel_dim});        c->set_output(0, feat_vox_shape);        return Status::OK();    })    .Doc(R"doc(The feature tsdf ops)doc");class FeatFusionVoxOp : public OpKernel {public:    explicit FeatFusionVoxOp(OpKernelConstruction* context) : OpKernel(context), vox_depth(0), vox_height(0),                                                              vox_width(0), img_height(0), img_width(0), channels(0)    {        int fusion_type_id = 0;        OP_REQUIRES_OK(context, context->GetAttr("fusion_type", &fusion_type_id));        OP_REQUIRES(context, fusion_type_id >= 0, errors::InvalidArgument("fusion type must be larger than 0"));        fusion_type = static_cast<FusionType>(fusion_type_id);    };    void Compute(OpKernelContext * context) override    {        // Assign the inputs arrangement        const Tensor& img_feat_input = context->input(0);        const Tensor& img_map_input = context->input(1);        const Tensor& img_weights_input = context->input(2);        const Tensor& vox_weights_input = context->input(3);        this->vox_depth = vox_weights_input.shape().dim_size(1);        this->vox_height = vox_weights_input.shape().dim_size(2);        this->vox_width = vox_weights_input.shape().dim_size(3);        this->img_height = img_feat_input.shape().dim_size(1);        this->img_width = img_feat_input.shape().dim_size(2);        this->channels = img_feat_input.dim_size(3);        // Assign the outputs layouts        int64 batch_size = img_feat_input.dim_size(0);        TensorShape vox_shape = TensorShape(gtl::ArraySlice<int64>{batch_size, vox_depth, vox_height, vox_width,                                                                   channels});        Tensor* vox_feat_output = nullptr;        OP_REQUIRES_OK(context, context->allocate_output("vox_feat", vox_shape, &vox_feat_output));        features_fusion(vox_feat_output, &img_feat_input, &img_map_input, &img_weights_input, &vox_weights_input);    }private:    void features_fusion(Tensor* vox_feat, const Tensor* img_feat, const Tensor* img_map, const Tensor* img_weights,                         const Tensor* vox_weights)    {        auto batch_size= img_map->dim_size(0);        auto vox_size = vox_depth * vox_height * vox_width;        auto img_size = img_height * img_width;        for (int64 batch_index = 0; batch_index < batch_size; batch_index++) {            auto vox_feat_ptr = vox_feat->Slice(batch_index, batch_index + 1).unaligned_flat<T>().data();            auto img_feat_ptr = img_feat->Slice(batch_index, batch_index + 1).unaligned_flat<T>().data();            auto img_map_ptr = img_map->Slice(batch_index, batch_index + 1).unaligned_flat<int>().data();            auto img_weights_ptr = img_weights->Slice(batch_index, batch_index + 1).unaligned_flat<T>().data();            auto vox_weights_ptr = vox_weights->Slice(batch_index, batch_index + 1).unaligned_flat<T>().data();            switch (fusion_type)            {                case FusionType::MAX_POOLING:                    im2vox_max_pooling_gpu(vox_feat_ptr, img_feat_ptr, img_map_ptr, (int)vox_size, (int)img_size, (int)channels);                    break;                case FusionType::AVERAGE_POOLING:                    im2vox_features_extra_gpu(vox_feat_ptr, img_feat_ptr, img_map_ptr, img_weights_ptr, vox_weights_ptr,                                              (int)vox_depth, (int)vox_height, (int)vox_width,                                              (int)img_height, (int)img_width,                                              (int)img_feat->dim_size(3), (int)vox_feat->dim_size(4), true);                    break;            }        }    }private:    int64 vox_depth, vox_height, vox_width;    int64 img_height, img_width;    int64 channels;    FusionType fusion_type;};REGISTER_KERNEL_BUILDER(Name("FeatFusionVox").Device(DEVICE_GPU), FeatFusionVoxOp);}